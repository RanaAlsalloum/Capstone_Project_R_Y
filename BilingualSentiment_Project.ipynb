{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a264d81",
   "metadata": {},
   "source": [
    "# Bilingual Sentiment Project (Arabic + English)\n",
    "\n",
    "هذا الدفتر يدرب نموذجين لتحليل المشاعر (عربي + إنجليزي) ثم يصدّرهما بصيغ:\n",
    "- `SavedModel/`\n",
    "- `.keras`\n",
    "- `.h5`\n",
    "- مع `tokenizer.json` + `label_map.json`\n",
    "\n",
    "> **المتطلبات المقترحة:**  \n",
    "> - TensorFlow >= 2.16 (يفضّل 2.20)  \n",
    "> - Python 3.10  \n",
    "> - pandas, numpy, scikit-learn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bcf11f5",
   "metadata": {},
   "source": [
    "## (اختياري) تثبيت الإصدارات المتوافقة"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ff14fc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If you installed packages here, please restart the kernel and run from top.\n"
     ]
    }
   ],
   "source": [
    "# شغّل خلية واحدة فقط حسب بيئتك ثم أعد تشغيل النواة (Kernel Restart).\n",
    "# خيار عام (Linux/Windows/Cloud):\n",
    "# !pip install -q --upgrade pip\n",
    "# !pip uninstall -y tensorflow tensorflow-cpu tensorflow-macos tensorflow-metal keras tensorboard protobuf grpcio\n",
    "# !pip install -q \"tensorflow==2.20.0\" \"pandas==2.2.2\" \"numpy==1.26.4\" \"scikit-learn==1.4.2\" \"h5py==3.11.0\"\n",
    "print(\"If you installed packages here, please restart the kernel and run from top.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72062c58",
   "metadata": {},
   "source": [
    "## 1) الاستيراد والإعداد العام"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "808150d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ranasalloum/anaconda3/envs/sentiment_env/lib/python3.10/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow: 2.16.1\n"
     ]
    }
   ],
   "source": [
    "import os, re, json\n",
    "from pathlib import Path\n",
    "import numpy as np, pandas as pd, tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer, tokenizer_from_json\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "print(\"TensorFlow:\", tf.__version__)\n",
    "\n",
    "# إعدادات عامة\n",
    "MAX_LEN   = 96\n",
    "NUM_WORDS = 20000\n",
    "CLASSES   = ['negative','neutral','positive']\n",
    "\n",
    "# مجلد الإخراج\n",
    "OUT_DIR = Path('bilingual_sentiment_model')\n",
    "(OUT_DIR/'ar').mkdir(parents=True, exist_ok=True)\n",
    "(OUT_DIR/'en').mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# كشف العربية ببساطة\n",
    "AR_RE = re.compile(r'[\\u0600-\\u06FF]')\n",
    "def detect_lang(s): \n",
    "    return \"ar\" if AR_RE.search(str(s)) else \"en\"\n",
    "\n",
    "# تطبيع خفيف للنص العربي\n",
    "AR_DIACRITICS = r\"[\\u0617-\\u061A\\u064B-\\u0652\\u0670]\"\n",
    "def ar_normalize(s: str) -> str:\n",
    "    s = str(s)\n",
    "    s = re.sub(AR_DIACRITICS, \"\", s)            # إزالة التشكيل\n",
    "    s = re.sub(r\"[ـ]+\", \"\", s)                  # إزالة التطويل\n",
    "    s = s.replace(\"أ\",\"ا\").replace(\"إ\",\"ا\").replace(\"آ\",\"ا\")\n",
    "    s = s.replace(\"ى\",\"ي\").replace(\"ؤ\",\"و\").replace(\"ئ\",\"ي\").replace(\"ة\",\"ه\")\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6635f4ae",
   "metadata": {},
   "source": [
    "## 2) تحميل CSV بذكاء (ترميزات وأسماء أعمدة مختلفة)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "828a9520",
   "metadata": {},
   "outputs": [],
   "source": [
    "# نحاول عدة ترميزات شائعة: utf-8 / utf-8-sig / latin-1 / cp1256\n",
    "def read_csv_any_encoding(path):\n",
    "    for enc in (\"utf-8\",\"utf-8-sig\",\"latin-1\",\"cp1256\"):\n",
    "        try:\n",
    "            return pd.read_csv(path, encoding=enc)\n",
    "        except UnicodeDecodeError:\n",
    "            continue\n",
    "    # أخيرًا بدون تحديد\n",
    "    return pd.read_csv(path)\n",
    "\n",
    "# مرونة في أسماء الأعمدة\n",
    "TEXT_COLS  = {'text','review','sentence','tweet','comment','message','content','body','clean_text','normalized_text','comment_text','translated_text','Text'}\n",
    "LABEL_COLS = {'label','labels','sentiment','target','class','polarity','emotion','category','y','rating','Label','Sentiment'}\n",
    "LANG_COLS  = {'lang','language','lang_id','lang_code','iso_lang','Language'}\n",
    "\n",
    "def pick_col(orig_cols, candidates):\n",
    "    mapping = {c.lower(): c for c in orig_cols}\n",
    "    # تطابق مباشر (case-insensitive)\n",
    "    for cand in candidates:\n",
    "        if cand in mapping: \n",
    "            return mapping[cand]\n",
    "    # تطابق يحتوي\n",
    "    for c in orig_cols:\n",
    "        lc = c.lower()\n",
    "        if any(cand in lc for cand in candidates):\n",
    "            return c\n",
    "    return None\n",
    "\n",
    "def normalize_label(v):\n",
    "    s = str(v).strip().lower()\n",
    "    # ترميزات رقمية شائعة\n",
    "    if s in {'-1','0','1','2'}:\n",
    "        return {'-1':'negative','0':'negative','1':'positive','2':'neutral'}.get(s)\n",
    "    # English\n",
    "    if s in {'neg','negative','bad'}: return 'negative'\n",
    "    if s in {'neu','neutral'}:       return 'neutral'\n",
    "    if s in {'pos','positive','good'}: return 'positive'\n",
    "    # Arabic\n",
    "    if s in {'سلبي','سلبية','سيء','سيئ','سئ'}: return 'negative'\n",
    "    if s in {'محايد','محايده'}: return 'neutral'\n",
    "    if s in {'ايجابي','إيجابي','ايجابية','إيجابية','جيد','ممتاز'}: return 'positive'\n",
    "    return None\n",
    "\n",
    "def load_concat_smart(files):\n",
    "    frames = []\n",
    "    for p in files:\n",
    "        if not os.path.exists(p):\n",
    "            print(f\"⚠️ Missing: {p} — skipping\")\n",
    "            continue\n",
    "        df = read_csv_any_encoding(p)\n",
    "        orig_cols = list(df.columns)\n",
    "        text_c  = pick_col(orig_cols, {c.lower() for c in TEXT_COLS})\n",
    "        label_c = pick_col(orig_cols, {c.lower() for c in LABEL_COLS})\n",
    "        lang_c  = pick_col(orig_cols, {c.lower() for c in LANG_COLS})\n",
    "\n",
    "        if text_c is None or label_c is None:\n",
    "            raise ValueError(f\"{p} lacks recognizable text/label columns. Found: {orig_cols}\")\n",
    "\n",
    "        df = df.rename(columns={text_c: 'text', label_c: 'label'})\n",
    "        if lang_c: \n",
    "            df = df.rename(columns={lang_c: 'lang'})\n",
    "        if 'lang' not in df.columns:\n",
    "            df['lang'] = df['text'].apply(detect_lang)\n",
    "\n",
    "        # تطبيع عربي للنصوص العربية فقط (اختياري لكنه يساعد)\n",
    "        mask_ar = df['lang'] == 'ar'\n",
    "        df.loc[mask_ar, 'text'] = df.loc[mask_ar, 'text'].astype(str).apply(ar_normalize)\n",
    "\n",
    "        # توحيد الليبل\n",
    "        df['label'] = df['label'].apply(normalize_label)\n",
    "        df = df.dropna(subset=['text','label','lang'])\n",
    "        df = df[df['label'].isin(CLASSES)]\n",
    "        frames.append(df[['text','label','lang']])\n",
    "        print(f\"✅ {p}: rows={len(df)} standardized.\")\n",
    "    if not frames:\n",
    "        raise ValueError(\"No valid CSVs found.\")\n",
    "    out = pd.concat(frames, ignore_index=True).drop_duplicates(subset=['text','label','lang'])\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2bdb988",
   "metadata": {},
   "source": [
    "### حمّل ملفاتك هنا"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8c2eade9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ train.csv: rows=27480 standardized.\n",
      "✅ train_all.csv: rows=55000 standardized.\n",
      "Total rows: 81728\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>lang</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>neutral</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n",
       "      <td>negative</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>my boss is bullying me...</td>\n",
       "      <td>negative</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>what interview! leave me alone</td>\n",
       "      <td>negative</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sons of ****, why couldn`t they put them on t...</td>\n",
       "      <td>negative</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>http://www.dothebouncy.com/smf - some shameles...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2am feedings for the baby are fun when he is a...</td>\n",
       "      <td>positive</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Soooo high</td>\n",
       "      <td>neutral</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Both of you</td>\n",
       "      <td>neutral</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Journey!? Wow... u just became cooler.  hehe....</td>\n",
       "      <td>positive</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text     label lang\n",
       "0                I`d have responded, if I were going   neutral   en\n",
       "1      Sooo SAD I will miss you here in San Diego!!!  negative   en\n",
       "2                          my boss is bullying me...  negative   en\n",
       "3                     what interview! leave me alone  negative   en\n",
       "4   Sons of ****, why couldn`t they put them on t...  negative   en\n",
       "5  http://www.dothebouncy.com/smf - some shameles...   neutral   en\n",
       "6  2am feedings for the baby are fun when he is a...  positive   en\n",
       "7                                         Soooo high   neutral   en\n",
       "8                                        Both of you   neutral   en\n",
       "9   Journey!? Wow... u just became cooler.  hehe....  positive   en"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# عدّل الأسماء حسب ملفاتك\n",
    "TRAIN_FILES = ['train.csv', 'train_all.csv']\n",
    "\n",
    "df = load_concat_smart(TRAIN_FILES)\n",
    "print(\"Total rows:\", len(df))\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11851b78",
   "metadata": {},
   "source": [
    "## 3) تحضير بيانات لكل لغة"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c538d859",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows AR: 54244 Rows EN: 27484\n",
      "AR shapes: (43395, 96) (10849, 96) EN shapes: (21987, 96) (5497, 96)\n"
     ]
    }
   ],
   "source": [
    "def build_lang_data(df_lang):\n",
    "    texts = df_lang['text'].astype(str).tolist()\n",
    "    labels = df_lang['label'].map({'negative':0,'neutral':1,'positive':2}).values\n",
    "    tok = Tokenizer(num_words=NUM_WORDS, oov_token='[OOV]')\n",
    "    tok.fit_on_texts(texts)\n",
    "    X = pad_sequences(tok.texts_to_sequences(texts), maxlen=MAX_LEN, padding='post', truncating='post')\n",
    "    y = labels\n",
    "    return X, y, tok\n",
    "\n",
    "def safe_split(X, y, test_size=0.2, random_state=42):\n",
    "    cnt = Counter(y)\n",
    "    strat = y if (len(cnt)>0 and min(cnt.values())>=2) else None\n",
    "    return train_test_split(X, y, test_size=test_size, random_state=random_state, shuffle=True, stratify=strat)\n",
    "\n",
    "df_ar = df[df.lang=='ar'].copy()\n",
    "df_en = df[df.lang=='en'].copy()\n",
    "print(\"Rows AR:\", len(df_ar), \"Rows EN:\", len(df_en))\n",
    "\n",
    "X_ar, y_ar, tok_ar = (np.empty((0,MAX_LEN)), np.array([]), None)\n",
    "X_en, y_en, tok_en = (np.empty((0,MAX_LEN)), np.array([]), None)\n",
    "\n",
    "if len(df_ar):\n",
    "    X_ar, y_ar, tok_ar = build_lang_data(df_ar)\n",
    "if len(df_en):\n",
    "    X_en, y_en, tok_en = build_lang_data(df_en)\n",
    "\n",
    "Xtr_ar, Xva_ar, ytr_ar, yva_ar = safe_split(X_ar, y_ar) if len(df_ar) else (X_ar,X_ar,y_ar,y_ar)\n",
    "Xtr_en, Xva_en, ytr_en, yva_en = safe_split(X_en, y_en) if len(df_en) else (X_en,X_en,y_en,y_en)\n",
    "\n",
    "print(\"AR shapes:\", Xtr_ar.shape, Xva_ar.shape, \"EN shapes:\", Xtr_en.shape, Xva_en.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dfd78d4",
   "metadata": {},
   "source": [
    "## 4) بناء الموديل"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "eea5aa69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(vocab_size=NUM_WORDS, num_classes=3, max_len=MAX_LEN):\n",
    "    inp = layers.Input(shape=(max_len,), dtype='int32')\n",
    "    x = layers.Embedding(vocab_size, 128, mask_zero=True)(inp)\n",
    "    x = layers.Bidirectional(layers.LSTM(64))(x)\n",
    "    x = layers.Dense(128, activation='relu')(x)\n",
    "    out = layers.Dense(num_classes, activation='softmax')(x)\n",
    "    model = models.Model(inp, out)\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "callbacks = [tf.keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True, monitor='val_accuracy')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf560a38",
   "metadata": {},
   "source": [
    "## 5) تدريب عربي + إنجليزي"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "13711046",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Rebuild helpers (right-padding=0) =====\n",
    "import numpy as np, pandas as pd, re\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "MAX_LEN   = 96\n",
    "NUM_WORDS = 20000\n",
    "CLASSES   = ['negative', 'neutral', 'positive']\n",
    "LABEL2ID  = {c:i for i,c in enumerate(CLASSES)}\n",
    "\n",
    "_AR_DIAC = r\"[\\u0617-\\u061A\\u064B-\\u0652\\u0670]\"\n",
    "def ar_norm(s):\n",
    "    s = str(s)\n",
    "    s = re.sub(_AR_DIAC, \"\", s)\n",
    "    s = re.sub(r\"[ـ]+\", \"\", s)\n",
    "    s = s.replace(\"أ\",\"ا\").replace(\"إ\",\"ا\").replace(\"آ\",\"ا\")\n",
    "    s = s.replace(\"ى\",\"ي\").replace(\"ؤ\",\"و\").replace(\"ئ\",\"ي\").replace(\"ة\",\"ه\")\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "def build_lang_data(df_lang: pd.DataFrame, is_ar: bool):\n",
    "    texts = df_lang['text'].astype(str).tolist()\n",
    "    if is_ar:\n",
    "        texts = [ar_norm(t) for t in texts]\n",
    "    y = df_lang['label'].map(LABEL2ID).values\n",
    "\n",
    "    tok = Tokenizer(num_words=NUM_WORDS, oov_token=\"<OOV>\")\n",
    "    tok.fit_on_texts(texts)\n",
    "    seqs = tok.texts_to_sequences(texts)\n",
    "    # right-padding بقيمة 0\n",
    "    X = pad_sequences(seqs, maxlen=MAX_LEN, padding=\"post\", truncating=\"post\", value=0)\n",
    "    return X, y, tok\n",
    "\n",
    "def assert_right_padded(X):\n",
    "    # يتأكد أن كل صف بعد آخر nonzero كله أصفار\n",
    "    bad = 0\n",
    "    for row in X:\n",
    "        nz = np.nonzero(row)[0]\n",
    "        if len(nz):\n",
    "            last = nz[-1]\n",
    "            if np.any(row[:np.max([0, last])] == 0):  # أصفار داخلية قبل آخر nonzero\n",
    "                bad += 1\n",
    "    if bad:\n",
    "        print(f\"⚠️ Found {bad} sequences with internal zeros (not strictly right-padded).\")\n",
    "    else:\n",
    "        print(\"✅ Sequences look right-padded.\")\n",
    "\n",
    "# ===== Model WITHOUT masks =====\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers as L\n",
    "\n",
    "def make_model(vocab_size=NUM_WORDS, num_classes=len(CLASSES), max_len=MAX_LEN, emb_dim=128):\n",
    "    inp = L.Input(shape=(max_len,), dtype=\"int32\")\n",
    "    # لا نستخدم mask_zero إطلاقًا\n",
    "    x = L.Embedding(vocab_size, emb_dim, mask_zero=False)(inp)\n",
    "    # نوقف cuDNN صراحة (حتى لو بدون ماسك)\n",
    "    x = L.Bidirectional(L.LSTM(64, return_sequences=True, use_cudnn=False))(x)\n",
    "    x = L.Bidirectional(L.LSTM(32, use_cudnn=False))(x)\n",
    "    x = L.Dense(64, activation=\"relu\")(x)\n",
    "    out = L.Dense(num_classes, activation=\"softmax\")(x)\n",
    "    model = keras.Model(inp, out)\n",
    "    model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d8c74eca-34e4-41b9-aca9-9b10da2596c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Sequences look right-padded.\n",
      "✅ Sequences look right-padded.\n"
     ]
    }
   ],
   "source": [
    "# افترضي أن df_ar و df_en جاهزين وبهم أعمدة: text, label\n",
    "X_ar, y_ar, tok_ar = build_lang_data(df_ar, is_ar=True)\n",
    "X_en, y_en, tok_en = build_lang_data(df_en, is_ar=False)\n",
    "\n",
    "assert_right_padded(X_ar)\n",
    "assert_right_padded(X_en)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "Xtr_ar, Xva_ar, ytr_ar, yva_ar = train_test_split(X_ar, y_ar, test_size=0.2, random_state=42, stratify=y_ar)\n",
    "Xtr_en, Xva_en, ytr_en, yva_en = train_test_split(X_en, y_en, test_size=0.2, random_state=42, stratify=y_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "093d6ab2-37ab-4985-9720-07fe5d72c8a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training AR model...\n",
      "Epoch 1/8\n",
      "1357/1357 - 172s - 127ms/step - accuracy: 0.7458 - loss: 0.6188 - val_accuracy: 0.7803 - val_loss: 0.5417 - learning_rate: 0.0010\n",
      "Epoch 2/8\n",
      "1357/1357 - 175s - 129ms/step - accuracy: 0.8311 - loss: 0.4126 - val_accuracy: 0.7734 - val_loss: 0.5591 - learning_rate: 0.0010\n",
      "Epoch 3/8\n",
      "1357/1357 - 185s - 137ms/step - accuracy: 0.8868 - loss: 0.2811 - val_accuracy: 0.7618 - val_loss: 0.6800 - learning_rate: 0.0010\n",
      "Epoch 4/8\n",
      "1357/1357 - 177s - 131ms/step - accuracy: 0.9420 - loss: 0.1495 - val_accuracy: 0.7476 - val_loss: 1.0228 - learning_rate: 5.0000e-04\n",
      "Training EN model...\n",
      "Epoch 1/8\n",
      "688/688 - 96s - 139ms/step - accuracy: 0.6331 - loss: 0.8102 - val_accuracy: 0.6973 - val_loss: 0.7206 - learning_rate: 0.0010\n",
      "Epoch 2/8\n",
      "688/688 - 91s - 132ms/step - accuracy: 0.7635 - loss: 0.5767 - val_accuracy: 0.7157 - val_loss: 0.6987 - learning_rate: 0.0010\n",
      "Epoch 3/8\n",
      "688/688 - 91s - 132ms/step - accuracy: 0.8455 - loss: 0.4081 - val_accuracy: 0.6938 - val_loss: 0.7460 - learning_rate: 5.0000e-04\n"
     ]
    }
   ],
   "source": [
    "callbacks = [\n",
    "    keras.callbacks.EarlyStopping(monitor=\"val_accuracy\", patience=3, restore_best_weights=True),\n",
    "    keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=2, min_lr=1e-5),\n",
    "]\n",
    "\n",
    "model_ar, model_en = None, None\n",
    "\n",
    "if len(Xtr_ar):\n",
    "    print(\"Training AR model...\")\n",
    "    model_ar = make_model()\n",
    "    model_ar.fit(Xtr_ar, ytr_ar, validation_data=(Xva_ar, yva_ar),\n",
    "                 epochs=8, batch_size=32, verbose=2, callbacks=callbacks)\n",
    "else:\n",
    "    print(\"⚠️ No Arabic samples — skipping AR training.\")\n",
    "\n",
    "if len(Xtr_en):\n",
    "    print(\"Training EN model...\")\n",
    "    model_en = make_model()\n",
    "    model_en.fit(Xtr_en, ytr_en, validation_data=(Xva_en, yva_en),\n",
    "                 epochs=8, batch_size=32, verbose=2, callbacks=callbacks)\n",
    "else:\n",
    "    print(\"⚠️ No English samples — skipping EN training.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "defe8f4f-787d-466d-9bc6-922e5777eccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_all(lang, model, tok, out_dir=\"bilingual_sentiment_model\"):\n",
    "    lang_dir = Path(out_dir) / lang\n",
    "    lang_dir.mkdir(parents=True, exist_ok=True)\n",
    "    # Save in multiple formats\n",
    "    model.save(lang_dir / f\"{lang}_best.keras\", include_optimizer=False)\n",
    "    model.save(lang_dir / f\"{lang}_best.h5\", include_optimizer=False)\n",
    "    model.export(lang_dir / \"saved_model\")   # للـ TensorFlow Serving / TFLite\n",
    "    # Save tokenizer + label map\n",
    "    (lang_dir / \"tokenizer.json\").write_text(tok.to_json(), encoding=\"utf-8\")\n",
    "    (lang_dir / \"label_map.json\").write_text(\n",
    "        json.dumps({\"classes\": CLASSES}, ensure_ascii=False, indent=2),\n",
    "        encoding=\"utf-8\"\n",
    "    )\n",
    "    print(f\"✅ Exported {lang} model to {lang_dir.resolve()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ec4b81-08a9-453d-86d9-03769ecec704",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training AR model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-09 19:43:09.860319: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M2\n",
      "2025-09-09 19:43:09.860489: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 8.00 GB\n",
      "2025-09-09 19:43:09.860501: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 2.67 GB\n",
      "2025-09-09 19:43:09.860688: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2025-09-09 19:43:09.860709: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-09 19:43:10.707279: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1357/1357 - 171s - 126ms/step - accuracy: 0.7486 - loss: 0.6172 - val_accuracy: 0.7656 - val_loss: 0.5633 - learning_rate: 0.0010\n",
      "Epoch 2/8\n",
      "1357/1357 - 181s - 133ms/step - accuracy: 0.8335 - loss: 0.4149 - val_accuracy: 0.7792 - val_loss: 0.5742 - learning_rate: 0.0010\n",
      "Epoch 3/8\n",
      "1357/1357 - 180s - 133ms/step - accuracy: 0.8908 - loss: 0.2779 - val_accuracy: 0.7531 - val_loss: 0.6795 - learning_rate: 0.0010\n",
      "Epoch 4/8\n"
     ]
    }
   ],
   "source": [
    "callbacks = [\n",
    "    keras.callbacks.EarlyStopping(monitor=\"val_accuracy\", patience=3, restore_best_weights=True),\n",
    "    keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=2, min_lr=1e-5),\n",
    "]\n",
    "\n",
    "model_ar, model_en = None, None\n",
    "tok_ar, tok_en = None, None  # هنا نخزن التوكنـايزر لكل لغة\n",
    "\n",
    "if len(Xtr_ar):\n",
    "    print(\"Training AR model...\")\n",
    "    model_ar = make_model()\n",
    "    model_ar.fit(\n",
    "        Xtr_ar, ytr_ar,\n",
    "        validation_data=(Xva_ar, yva_ar),\n",
    "        epochs=8, batch_size=32, verbose=2,\n",
    "        callbacks=callbacks\n",
    "    )\n",
    "    tok_ar = tok_ar  # هذا لازم يكون من مرحلة الـ preprocessing\n",
    "else:\n",
    "    print(\"⚠️ No Arabic samples — skipping AR training.\")\n",
    "\n",
    "if len(Xtr_en):\n",
    "    print(\"Training EN model...\")\n",
    "    model_en = make_model()\n",
    "    model_en.fit(\n",
    "        Xtr_en, ytr_en,\n",
    "        validation_data=(Xva_en, yva_en),\n",
    "        epochs=8, batch_size=32, verbose=2,\n",
    "        callbacks=callbacks\n",
    "    )\n",
    "    tok_en = tok_en  # نفس الشيء هنا\n",
    "else:\n",
    "    print(\"⚠️ No English samples — skipping EN training.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4267695f",
   "metadata": {},
   "source": [
    "## 6) التصدير (SavedModel + .keras + .h5 + tokenizer/label_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "fdd8c2df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: bilingual_sentiment_model/ar/saved_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: bilingual_sentiment_model/ar/saved_model/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved artifact at 'bilingual_sentiment_model/ar/saved_model'. The following endpoints are available:\n",
      "\n",
      "* Endpoint 'serve'\n",
      "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 96), dtype=tf.int32, name='keras_tensor_68')\n",
      "Output Type:\n",
      "  TensorSpec(shape=(None, 3), dtype=tf.float32, name=None)\n",
      "Captures:\n",
      "  13101421440: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  14634859360: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  14634857776: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  14634860240: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  14634845104: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  14634857248: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  14634858304: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  6326205632: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  6326203168: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  6326205808: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  6325997232: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  6325997408: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  6325996352: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  6325999520: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  6325998288: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  6326002160: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  6326002864: TensorSpec(shape=(), dtype=tf.resource, name=None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Exported ar → /Users/ranasalloum/000/bilingual_sentiment_model/ar\n",
      "INFO:tensorflow:Assets written to: bilingual_sentiment_model/en/saved_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: bilingual_sentiment_model/en/saved_model/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved artifact at 'bilingual_sentiment_model/en/saved_model'. The following endpoints are available:\n",
      "\n",
      "* Endpoint 'serve'\n",
      "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 96), dtype=tf.int32, name='keras_tensor_74')\n",
      "Output Type:\n",
      "  TensorSpec(shape=(None, 3), dtype=tf.float32, name=None)\n",
      "Captures:\n",
      "  6327241184: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  6326034928: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  13247643344: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  13247642464: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  13247627504: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  13247638944: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  13247635424: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  6139727184: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  6398892288: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  6398891584: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  6398894752: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  6398900208: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  6398900736: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  12934905376: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  12934893936: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  12934893584: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  12934894640: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "✅ Exported en → /Users/ranasalloum/000/bilingual_sentiment_model/en\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "OUT_DIR = Path(\"bilingual_sentiment_model\")\n",
    "CLASSES = [\"negative\", \"neutral\", \"positive\"]  # ثبتيها حسب بياناتك\n",
    "\n",
    "def export_all(lang, model, tok):\n",
    "    lang_dir = OUT_DIR / lang\n",
    "    lang_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # 1) Keras format (.keras)\n",
    "    model.save(lang_dir / f\"{lang}_best.keras\", include_optimizer=False)\n",
    "\n",
    "    # 2) H5 format (.h5)\n",
    "    model.save(lang_dir / f\"{lang}_best.h5\", include_optimizer=False)\n",
    "\n",
    "    # 3) SavedModel (في Keras 3 نستخدم export بدل save)\n",
    "    sm_dir = lang_dir / \"saved_model\"\n",
    "    sm_dir.mkdir(exist_ok=True)\n",
    "    model.export(str(sm_dir))\n",
    "\n",
    "    # 4) Tokenizer & Label Map\n",
    "    (lang_dir / \"tokenizer.json\").write_text(tok.to_json(), encoding=\"utf-8\")\n",
    "    (lang_dir / \"label_map.json\").write_text(\n",
    "        json.dumps({\"classes\": CLASSES}, ensure_ascii=False, indent=2),\n",
    "        encoding=\"utf-8\"\n",
    "    )\n",
    "\n",
    "    print(f\"✅ Exported {lang} → {lang_dir.resolve()}\")\n",
    "\n",
    "# حفظ العربي والإنجليزي إذا متوفرين\n",
    "if model_ar is not None and tok_ar is not None:\n",
    "    export_all(\"ar\", model_ar, tok_ar)\n",
    "\n",
    "if model_en is not None and tok_en is not None:\n",
    "    export_all(\"en\", model_en, tok_en)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6647041c",
   "metadata": {},
   "source": [
    "## 7) اختبار سريع للتوقع + موجه توجيه (Router)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "62ea0272",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'lang': 'ar', 'label': 'neutral', 'confidence': 0.6686696410179138}\n",
      "{'lang': 'en', 'label': 'negative', 'confidence': 0.8545156717300415}\n"
     ]
    }
   ],
   "source": [
    "def load_assets_for_infer(lang_dir: Path):\n",
    "    # tokenizer\n",
    "    with open(lang_dir / 'tokenizer.json', 'r', encoding='utf-8') as f:\n",
    "        tok = tokenizer_from_json(f.read())\n",
    "    # label map\n",
    "    with open(lang_dir / 'label_map.json', 'r', encoding='utf-8') as f:\n",
    "        classes = json.load(f)['classes']\n",
    "    # model\n",
    "    # جرّب .keras ثم .h5 ثم SavedModel\n",
    "    for cand in [lang_dir / f\"{lang_dir.name}_best.keras\", lang_dir / f\"{lang_dir.name}_best.h5\", lang_dir / 'saved_model']:\n",
    "        if cand.exists():\n",
    "            model = tf.keras.models.load_model(cand, compile=False)\n",
    "            return tok, classes, model\n",
    "    raise FileNotFoundError(f\"No model found under {lang_dir}\")\n",
    "\n",
    "def infer_one(text, model_root='bilingual_sentiment_model'):\n",
    "    text = str(text)\n",
    "    lang = 'ar' if AR_RE.search(text) else 'en'\n",
    "    # normalize arabic\n",
    "    if lang=='ar':\n",
    "        text = ar_normalize(text)\n",
    "    tok, classes, model = load_assets_for_infer(Path(model_root)/lang)\n",
    "    X = pad_sequences(tok.texts_to_sequences([text]), maxlen=MAX_LEN, padding='post', truncating='post')\n",
    "    probs = model.predict(X, verbose=0)[0]\n",
    "    idx = int(np.argmax(probs))\n",
    "    return {'lang': lang, 'label': classes[idx], 'confidence': float(probs[idx])}\n",
    "\n",
    "# تجربة\n",
    "try:\n",
    "    print(infer_one(\"انا مبسوط اليوم\"))\n",
    "    print(infer_one(\"I hate this product\"))\n",
    "except Exception as e:\n",
    "    print(\"Inference skipped:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18ebaba-a3b7-4ae8-bdfb-b740dc39d2ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "! streamlit run app.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa7667fe-2291-4856-aed3-c2706742f2bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2dde62-a273-404a-be7a-bc62a3a774d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:sentiment_env]",
   "language": "python",
   "name": "conda-env-sentiment_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
