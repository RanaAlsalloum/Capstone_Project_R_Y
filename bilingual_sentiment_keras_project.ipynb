{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70367437",
   "metadata": {},
   "source": [
    "# Bilingual (Arabic + English) Sentiment — Keras Baseline\n",
    "\n",
    "This notebook trains **two separate models**:\n",
    "- **Arabic model**: trained on `train_all_ext.csv` + `train_all.csv` (columns: `Text`, `sentiment`).\n",
    "- **English model**: trained on `train.csv` (columns: `text`, `sentiment`, with `latin-1` encoding).\n",
    "\n",
    "Each model is saved in **both** `.keras` and `.h5` formats, and we export `tokenizer.json`, `label_map.json`, and metrics.\n",
    "\n",
    "At the end, an **inference router** auto-detects language and uses the correct model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d79103f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: install dependencies (uncomment if needed)\n",
    "#!pip install tensorflow==2.15.0 keras==2.15.0 pandas numpy\n",
    "# For Apple Silicon:\n",
    "# !pip install tensorflow-macos==2.15.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ccecde6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ranasalloum/anaconda3/envs/resume/lib/python3.10/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.15.0\n"
     ]
    }
   ],
   "source": [
    "import os, json, re, csv, warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Dense, Dropout, GlobalMaxPool1D\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea96f8de",
   "metadata": {},
   "source": [
    "## Paths & Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0c20967b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust paths if your files are elsewhere\n",
    "AR_CSVS = [\n",
    "    \"train_all_ext.csv\",  # (Text, sentiment)\n",
    "    \"train_all.csv\",      # (Text, sentiment)\n",
    "]\n",
    "EN_CSV  = \"train.csv\"     # (text, sentiment) encoded latin-1\n",
    "\n",
    "OUT_DIR    = \"bilingual_sentiment_model\"\n",
    "EPOCHS     = 5\n",
    "BATCH_SIZE = 64\n",
    "MAX_WORDS  = 20000\n",
    "MAX_LEN    = 96\n",
    "SEED       = 42\n",
    "\n",
    "Path(OUT_DIR).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "136cbcd9",
   "metadata": {},
   "source": [
    "## Utilities: Loading, Cleaning, Tokenization, Model Builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b053bec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ARABIC_RE = re.compile(r'[\\u0600-\\u06FF]')\n",
    "\n",
    "def read_csv_safe(path, encoding=None):\n",
    "    encs = ([encoding] if encoding else []) + [\"utf-8\", \"utf-8-sig\", \"latin-1\", \"cp1252\", \"windows-1252\"]\n",
    "    last_err = None\n",
    "    for enc in encs:\n",
    "        try:\n",
    "            return pd.read_csv(path, encoding=enc)\n",
    "        except Exception as e:\n",
    "            last_err = e\n",
    "    raise last_err\n",
    "\n",
    "def load_arabic_df(csvs):\n",
    "    frames = []\n",
    "    for p in csvs:\n",
    "        if not os.path.exists(p):\n",
    "            print(f\"[WARN] Arabic CSV not found: {p}\")\n",
    "            continue\n",
    "        df = read_csv_safe(p)\n",
    "        if 'Text' not in df.columns or 'sentiment' not in df.columns:\n",
    "            print(f\"[WARN] {p} missing required columns. Found: {list(df.columns)}\")\n",
    "            continue\n",
    "        df = df[['Text','sentiment']].dropna()\n",
    "        df = df[df['Text'].astype(str).str.contains(ARABIC_RE)]\n",
    "        frames.append(df.rename(columns={'Text':'text'}))\n",
    "    if not frames:\n",
    "        raise ValueError(\"No valid Arabic frames loaded.\")\n",
    "    out = pd.concat(frames, ignore_index=True)\n",
    "    out['lang'] = 'ar'\n",
    "    return out\n",
    "\n",
    "def load_english_df(csv_path):\n",
    "    if not os.path.exists(csv_path):\n",
    "        raise ValueError(f\"English CSV not found: {csv_path}\")\n",
    "    df = read_csv_safe(csv_path, encoding='latin-1')\n",
    "    if 'text' not in df.columns or 'sentiment' not in df.columns:\n",
    "        raise ValueError(f\"English CSV missing required cols. Found: {list(df.columns)}\")\n",
    "    df = df[['text','sentiment']].dropna()\n",
    "    df = df[~df['text'].astype(str).str.contains(ARABIC_RE)]\n",
    "    df['lang'] = 'en'\n",
    "    return df\n",
    "\n",
    "def clean_text_basic(s: str, lang: str):\n",
    "    s = str(s)\n",
    "    s = re.sub(r\"http\\S+|www\\.\\S+|@\\w+|#\\w+\", \" \", s)\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    if lang == 'en':\n",
    "        s = s.lower()\n",
    "    return s\n",
    "\n",
    "def prepare_xy(df, max_words, max_len):\n",
    "    texts  = df['text'].astype(str).tolist()\n",
    "    labels = df['sentiment'].astype(str).tolist()\n",
    "    classes = sorted(list(dict.fromkeys(labels)))\n",
    "    cls2idx = {c:i for i,c in enumerate(classes)}\n",
    "    y = np.array([cls2idx[c] for c in labels])\n",
    "    tok = Tokenizer(num_words=max_words, oov_token='<OOV>')\n",
    "    tok.fit_on_texts(texts)\n",
    "    X_seq = tok.texts_to_sequences(texts)\n",
    "    X = pad_sequences(X_seq, maxlen=max_len, padding='post', truncating='post')\n",
    "    return X, y, tok, classes\n",
    "\n",
    "def build_model(vocab, max_len, num_classes):\n",
    "    model = Sequential([\n",
    "        Embedding(input_dim=vocab, output_dim=128, input_length=max_len),\n",
    "        Bidirectional(LSTM(64, return_sequences=True)),\n",
    "        GlobalMaxPool1D(),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dropout(0.3),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def train_one_lang(df, lang, out_dir, max_words, max_len, epochs, batch_size, seed=42):\n",
    "    df = df.copy()\n",
    "    df['text'] = df.apply(lambda r: clean_text_basic(r['text'], lang), axis=1)\n",
    "\n",
    "    rng = np.random.default_rng(seed)\n",
    "    idx = np.arange(len(df))\n",
    "    rng.shuffle(idx)\n",
    "    df = df.iloc[idx].reset_index(drop=True)\n",
    "\n",
    "    n = len(df)\n",
    "    n_tr = int(n*0.9)\n",
    "    df_tr = df.iloc[:n_tr]\n",
    "    df_va = df.iloc[n_tr:]\n",
    "\n",
    "    X_tr, y_tr, tok, classes = prepare_xy(df_tr, max_words, max_len)\n",
    "    X_va = pad_sequences(tok.texts_to_sequences(df_va['text'].astype(str).tolist()), maxlen=max_len, padding='post', truncating='post')\n",
    "    y_va = np.array([classes.index(s) for s in df_va['sentiment'].astype(str).tolist()])\n",
    "\n",
    "    vocab = min(max_words, len(tok.word_index)+1)\n",
    "    model = build_model(vocab, max_len, num_classes=len(classes))\n",
    "\n",
    "    lang_dir = Path(out_dir) / lang\n",
    "    lang_dir.mkdir(parents=True, exist_ok=True)\n",
    "    ckpt_path = str(lang_dir / f\"{lang}_best.keras\")\n",
    "    ckpt = ModelCheckpoint(ckpt_path, monitor='val_accuracy', save_best_only=True, verbose=1)\n",
    "    es = EarlyStopping(monitor='val_accuracy', patience=3, restore_best_weights=True, verbose=1)\n",
    "\n",
    "    hist = model.fit(\n",
    "        X_tr, y_tr,\n",
    "        validation_data=(X_va, y_va),\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        callbacks=[ckpt, es],\n",
    "        verbose=2\n",
    "    )\n",
    "\n",
    "    keras_path = lang_dir / f\"{lang}_final.keras\"\n",
    "    h5_path    = lang_dir / f\"{lang}_final.h5\"\n",
    "    model.save(keras_path)\n",
    "    try:\n",
    "        model.save(h5_path, save_format='h5')\n",
    "    except Exception:\n",
    "        model.save(h5_path)\n",
    "\n",
    "    tok_json = tok.to_json()\n",
    "    with open(lang_dir / 'tokenizer.json', 'w', encoding='utf-8') as f:\n",
    "        f.write(tok_json)\n",
    "    with open(lang_dir / 'label_map.json', 'w', encoding='utf-8') as f:\n",
    "        json.dump({'classes': classes}, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    hist_csv = lang_dir / 'history.csv'\n",
    "    keys = sorted(hist.history.keys())\n",
    "    with open(hist_csv, 'w', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(['epoch'] + keys)\n",
    "        for i in range(len(hist.history[keys[0]])):\n",
    "            writer.writerow([i] + [hist.history[k][i] for k in keys])\n",
    "\n",
    "    metrics = {\n",
    "        'train_size': int(len(df_tr)),\n",
    "        'val_size': int(len(df_va)),\n",
    "        'best_val_acc': float(max(hist.history.get('val_accuracy', [0]))),\n",
    "        'classes': classes,\n",
    "    }\n",
    "    with open(lang_dir / 'metrics.json', 'w', encoding='utf-8') as f:\n",
    "        json.dump(metrics, f, ensure_ascii=False, indent=2)\n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d48fe2af",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2d7108db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Loading Arabic data ===\n",
      "Arabic rows: 128606\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>lang</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>الزعل بيغير ملامحك  بيغير نظرة العين  بيغير شك...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>ar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@halgawi @DmfMohe ليس حباً في ايران بقدر ماهو ...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>ar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@adalfahadduwail أبي أعرف الحاكم العربي المسلم...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>ar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@sarmadbouchamou @DimaSadek في الخطاب تبع سليم...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>ar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@FofaMahmouddd مفيش الكلام ده في الزمن</td>\n",
       "      <td>neutral</td>\n",
       "      <td>ar</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text sentiment lang\n",
       "0  الزعل بيغير ملامحك  بيغير نظرة العين  بيغير شك...   neutral   ar\n",
       "1  @halgawi @DmfMohe ليس حباً في ايران بقدر ماهو ...   neutral   ar\n",
       "2  @adalfahadduwail أبي أعرف الحاكم العربي المسلم...   neutral   ar\n",
       "3  @sarmadbouchamou @DimaSadek في الخطاب تبع سليم...   neutral   ar\n",
       "4             @FofaMahmouddd مفيش الكلام ده في الزمن   neutral   ar"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Loading English data ===\n",
      "English rows: 27480\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>lang</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>neutral</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n",
       "      <td>negative</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>my boss is bullying me...</td>\n",
       "      <td>negative</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>what interview! leave me alone</td>\n",
       "      <td>negative</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sons of ****, why couldn`t they put them on t...</td>\n",
       "      <td>negative</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text sentiment lang\n",
       "0                I`d have responded, if I were going   neutral   en\n",
       "1      Sooo SAD I will miss you here in San Diego!!!  negative   en\n",
       "2                          my boss is bullying me...  negative   en\n",
       "3                     what interview! leave me alone  negative   en\n",
       "4   Sons of ****, why couldn`t they put them on t...  negative   en"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"=== Loading Arabic data ===\")\n",
    "ar_df = load_arabic_df(AR_CSVS)\n",
    "print(\"Arabic rows:\", len(ar_df))\n",
    "display(ar_df.head())\n",
    "\n",
    "print(\"=== Loading English data ===\")\n",
    "en_df = load_english_df(EN_CSV)\n",
    "print(\"English rows:\", len(en_df))\n",
    "display(en_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe18f0b",
   "metadata": {},
   "source": [
    "## Train Arabic Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "caa5d8ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\n",
      "Epoch 1: val_accuracy improved from -inf to 0.83664, saving model to bilingual_sentiment_model/ar/ar_best.keras\n",
      "1809/1809 - 220s - loss: 0.5251 - accuracy: 0.7920 - val_loss: 0.4092 - val_accuracy: 0.8366 - 220s/epoch - 121ms/step\n",
      "Epoch 2/5\n",
      "\n",
      "Epoch 2: val_accuracy improved from 0.83664 to 0.87528, saving model to bilingual_sentiment_model/ar/ar_best.keras\n",
      "1809/1809 - 217s - loss: 0.3007 - accuracy: 0.8864 - val_loss: 0.3360 - val_accuracy: 0.8753 - 217s/epoch - 120ms/step\n",
      "Epoch 3/5\n",
      "\n",
      "Epoch 3: val_accuracy improved from 0.87528 to 0.89931, saving model to bilingual_sentiment_model/ar/ar_best.keras\n",
      "1809/1809 - 216s - loss: 0.1769 - accuracy: 0.9361 - val_loss: 0.3133 - val_accuracy: 0.8993 - 216s/epoch - 120ms/step\n",
      "Epoch 4/5\n",
      "\n",
      "Epoch 4: val_accuracy improved from 0.89931 to 0.91836, saving model to bilingual_sentiment_model/ar/ar_best.keras\n",
      "1809/1809 - 307s - loss: 0.1041 - accuracy: 0.9630 - val_loss: 0.2948 - val_accuracy: 0.9184 - 307s/epoch - 170ms/step\n",
      "Epoch 5/5\n",
      "\n",
      "Epoch 5: val_accuracy improved from 0.91836 to 0.92948, saving model to bilingual_sentiment_model/ar/ar_best.keras\n",
      "1809/1809 - 300s - loss: 0.0629 - accuracy: 0.9766 - val_loss: 0.3482 - val_accuracy: 0.9295 - 300s/epoch - 166ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ranasalloum/anaconda3/envs/resume/lib/python3.10/site-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'train_size': 115745,\n",
       " 'val_size': 12861,\n",
       " 'best_val_acc': 0.9294767379760742,\n",
       " 'classes': ['negative', 'neutral', 'positive']}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ar_metrics = train_one_lang(\n",
    "    ar_df, 'ar', OUT_DIR,\n",
    "    max_words=MAX_WORDS, max_len=MAX_LEN,\n",
    "    epochs=EPOCHS, batch_size=BATCH_SIZE, seed=SEED\n",
    ")\n",
    "ar_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "233fbabd",
   "metadata": {},
   "source": [
    "## Train English Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e281ffa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\n",
      "Epoch 1: val_accuracy improved from -inf to 0.72817, saving model to bilingual_sentiment_model/en/en_best.keras\n",
      "387/387 - 74s - loss: 0.8177 - accuracy: 0.6230 - val_loss: 0.6552 - val_accuracy: 0.7282 - 74s/epoch - 192ms/step\n",
      "Epoch 2/5\n",
      "\n",
      "Epoch 2: val_accuracy did not improve from 0.72817\n",
      "387/387 - 77s - loss: 0.5537 - accuracy: 0.7780 - val_loss: 0.6685 - val_accuracy: 0.7202 - 77s/epoch - 199ms/step\n",
      "Epoch 3/5\n",
      "\n",
      "Epoch 3: val_accuracy did not improve from 0.72817\n",
      "387/387 - 74s - loss: 0.4081 - accuracy: 0.8472 - val_loss: 0.7641 - val_accuracy: 0.7194 - 74s/epoch - 191ms/step\n",
      "Epoch 4/5\n",
      "\n",
      "Epoch 4: val_accuracy did not improve from 0.72817\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "387/387 - 79s - loss: 0.2997 - accuracy: 0.8941 - val_loss: 0.8294 - val_accuracy: 0.7107 - 79s/epoch - 204ms/step\n",
      "Epoch 4: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ranasalloum/anaconda3/envs/resume/lib/python3.10/site-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'train_size': 24732,\n",
       " 'val_size': 2748,\n",
       " 'best_val_acc': 0.7281659245491028,\n",
       " 'classes': ['negative', 'neutral', 'positive']}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_metrics = train_one_lang(\n",
    "    en_df, 'en', OUT_DIR,\n",
    "    max_words=MAX_WORDS, max_len=MAX_LEN,\n",
    "    epochs=EPOCHS, batch_size=BATCH_SIZE, seed=SEED\n",
    ")\n",
    "en_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b357e531",
   "metadata": {},
   "source": [
    "## Save Top-Level Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7f4a4674",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ar_metrics' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m summary \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mar\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[43mar_metrics\u001b[49m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124men\u001b[39m\u001b[38;5;124m'\u001b[39m: en_metrics}\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(Path(OUT_DIR)\u001b[38;5;241m/\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msummary.json\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m      3\u001b[0m     json\u001b[38;5;241m.\u001b[39mdump(summary, f, ensure_ascii\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, indent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ar_metrics' is not defined"
     ]
    }
   ],
   "source": [
    "summary = {'ar': ar_metrics, 'en': en_metrics}\n",
    "with open(Path(OUT_DIR)/'summary.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(summary, f, ensure_ascii=False, indent=2)\n",
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c61b2bec",
   "metadata": {},
   "source": [
    "## Inference Router (Auto Language Detect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3ceb2e98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('en', 'positive', 0.9262523651123047), ('ar', 'positive', 0.786483645439148), ('en', 'negative', 0.8367998600006104), ('ar', 'neutral', 0.9998291730880737)]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import tokenizer_from_json\n",
    "\n",
    "def detect_language_simple(text: str) -> str:\n",
    "    return 'ar' if ARABIC_RE.search(str(text)) else 'en'\n",
    "\n",
    "def load_lang_assets(model_dir, lang, max_len=96):\n",
    "    lang_dir = Path(model_dir) / lang\n",
    "    with open(lang_dir / 'tokenizer.json', 'r', encoding='utf-8') as f:\n",
    "        tok = tokenizer_from_json(f.read())\n",
    "    with open(lang_dir / 'label_map.json', 'r', encoding='utf-8') as f:\n",
    "        classes = json.load(f)['classes']\n",
    "    model_path = lang_dir / f\"{lang}_best.keras\"\n",
    "    if not model_path.exists():\n",
    "        model_path = lang_dir / f\"{lang}_final.keras\"\n",
    "    model = tf.keras.models.load_model(model_path)\n",
    "    return tok, classes, model, max_len\n",
    "\n",
    "def predict_texts(texts, model_dir=OUT_DIR):\n",
    "    groups = {'ar': [], 'en': []}\n",
    "    idxs = {'ar': [], 'en': []}\n",
    "    for i, t in enumerate(texts):\n",
    "        lang = detect_language_simple(t)\n",
    "        groups[lang].append(t)\n",
    "        idxs[lang].append(i)\n",
    "    outputs = [None]*len(texts)\n",
    "    for lang in ['ar','en']:\n",
    "        if not groups[lang]:\n",
    "            continue\n",
    "        tok, classes, model, max_len = load_lang_assets(model_dir, lang)\n",
    "        seqs = tok.texts_to_sequences(groups[lang])\n",
    "        X = pad_sequences(seqs, maxlen=max_len, padding='post', truncating='post')\n",
    "        probs = model.predict(X, verbose=0)\n",
    "        preds = probs.argmax(axis=1)\n",
    "        for j, i_orig in enumerate(idxs[lang]):\n",
    "            outputs[i_orig] = (lang, classes[preds[j]], float(probs[j][preds[j]]))\n",
    "    return outputs\n",
    "\n",
    "# Demo (run after training):\n",
    "examples = [\"I love this course!\", \"اليوم جميل\", \"This is terrible\", \"مرة مبسوط\"]\n",
    "try:\n",
    "    print(predict_texts(examples))\n",
    "except Exception as e:\n",
    "    print(\"(Run after training):\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4d478c-eef6-46d9-83d4-0af94ecba478",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\n",
      "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[34m  Local URL: \u001b[0m\u001b[1mhttp://localhost:8502\u001b[0m\n",
      "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://192.168.7.94:8502\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[34m\u001b[1m  For better performance, install the Watchdog module:\u001b[0m\n",
      "\n",
      "  $ xcode-select --install\n",
      "  $ pip install watchdog\n",
      "            \u001b[0m\n",
      "[mutex.cc : 452] RAW: Lock blocking 0x600001cdcbb8   @\n"
     ]
    }
   ],
   "source": [
    "! streamlit run app12.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4874b04-eec6-4fa0-8ff9-0327cb2d8981",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:resume]",
   "language": "python",
   "name": "conda-env-resume-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
