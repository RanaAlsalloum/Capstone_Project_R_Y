# app.py â€” Bilingual (Arabic + English) Sentiment | ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø´Ø§Ø¹Ø±
# ÙŠØ¯Ø¹Ù…: Ù†Øµ ÙˆØ§Ø­Ø¯ØŒ CSVØŒ PDFØŒ DOCX + Ø¥Ø¯Ø§Ø±Ø© Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„Ø§Øª + ÙØ­Øµ Ø§Ù„Ø¨ÙŠØ¦Ø©

import os, sys, re, io, zipfile, json
from pathlib import Path
from typing import Dict, Any, Tuple, List

import numpy as np
import pandas as pd
import streamlit as st

# Ù…ÙƒØªØ¨Ø§Øª Ø§Ù„Ù…Ù„ÙØ§Øª
from pypdf import PdfReader            # pip install pypdf
from docx import Document              # pip install python-docx

# ML
import tensorflow as tf
from tensorflow.keras.preprocessing.text import tokenizer_from_json
from tensorflow.keras.preprocessing.sequence import pad_sequences

# ------------------------
# Page & Globals
# ------------------------
st.set_page_config(page_title="ğŸ’¬ Sentiment | ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø´Ø§Ø¹Ø±", page_icon="ğŸ’¬", layout="wide")
DEFAULT_MODEL_DIR = Path("bilingual_sentiment_model")
MAX_LEN = 96
CLASSES_FALLBACK = ["negative", "neutral", "positive"]  # Ù„Ùˆ label_map.json Ù…ÙÙ‚ÙˆØ¯

# ------------------------
# Lang utils
# ------------------------
ARABIC_RE = re.compile(r'[\u0600-\u06FF]')
AR_DIACRITICS = r"[\u0617-\u061A\u064B-\u0652\u0670]"

def detect_language_simple(text: str) -> str:
    return "ar" if ARABIC_RE.search(str(text)) else "en"

def ar_normalize(s: str) -> str:
    s = str(s)
    s = re.sub(AR_DIACRITICS, "", s)
    s = re.sub(r"[Ù€]+", "", s)
    s = s.replace("Ø£","Ø§").replace("Ø¥","Ø§").replace("Ø¢","Ø§")
    s = s.replace("Ù‰","ÙŠ").replace("Ø¤","Ùˆ").replace("Ø¦","ÙŠ").replace("Ø©","Ù‡")
    s = re.sub(r"\s+", " ", s).strip()
    return s

def preprocess_text(txt: str, lang: str) -> str:
    return ar_normalize(txt) if lang == "ar" else txt

# ------------------------
# Arabic keyword override (ÙŠØ­Ø±Ùƒ Ø§Ù„Ø­Ø§Ù„Ø§Øª Ø§Ù„Ù…Ø­Ø§ÙŠØ¯Ø© Ø§Ù„Ù‚Ø±ÙŠØ¨Ø©)
# ------------------------
AR_NEG = {
    "Ø­Ø²ÙŠÙ†","Ø²Ø¹Ù„Ø§Ù†","ØªØ¹ÙŠØ³","Ø³ÙŠØ¦","Ø³ÙŠØ¡","Ù…ÙƒØªØ¦Ø¨","Ù…Ø­Ø¨Ø·","ØªØ¹Ø¨Ø§Ù†","ÙƒØ§Ø±Ù‡","Ù…Ø²Ø¹Ø¬","Ø±Ø¯ÙŠØ¡","Ø³Ø¦",
    "ÙƒØ§Ø±Ø«ÙŠ","Ù…Ù‚Ø±Ù","ÙØ¸ÙŠØ¹","Ø³ÙŠØ¦Ø©","Ø²ÙØª","ØºØ«ÙŠØ«","Ù…Ø¤Ø³Ù","Ù…Ø®ÙŠÙ‘Ø¨","Ø£Ø³ÙˆØ£","Ø£Ø¨Ø¯Ø§Ù‹ Ù…Ø§ Ø¹Ø¬Ø¨Ù†ÙŠ","Ù…Ù…Ù„"
}
AR_POS = {
    "Ø³Ø¹ÙŠØ¯","Ù…Ø¨Ø³ÙˆØ·","ÙØ±Ø­Ø§Ù†","Ù…Ù…ØªØ§Ø²","Ø±Ø§Ø¦Ø¹","Ø¬Ù…ÙŠÙ„","Ø­Ù„Ùˆ","Ø§Ø­Ø¨","Ø£Ø­Ø¨","Ø¹Ø¬Ø¨Ù†ÙŠ","Ù…Ø°Ù‡Ù„",
    "Ù…Ø³Ø¹Ø¯","Ù‡Ø§ÙŠÙ„","ÙƒÙˆÙŠØ³","Ù…Ù…ØªØ§Ø²Ù‡","ØªØ­ÙÙ‡","Ø®ÙŠØ§Ù„ÙŠ","ÙŠÙÙˆØ²","Ø­Ø¨ÙŠØª","Ø£ÙØ¶Ù„","Ù…Ø±Ø¶ÙŠ","Ù…Ø¨Ù‡Ø±"
}

def override_ar_prediction(text: str, label: str, probs: np.ndarray, classes: List[str], margin: float = 0.15) -> str:
    # Ù„Ø§ ØªØ¹Ù…Ù„ Ø¥Ù„Ø§ Ø¥Ø°Ø§ Ø¹Ù†Ø¯Ù†Ø§ ØµÙ†Ù "neutral"
    if "neutral" not in classes: 
        return label
    try:
        i_neu = classes.index("neutral")
        i_neg = classes.index("negative")
        i_pos = classes.index("positive")
    except ValueError:
        return label

    t = str(text)
    has_neg = any(w in t for w in AR_NEG)
    has_pos = any(w in t for w in AR_POS)

    if label == "neutral":
        # Ù‚Ø±Ù‘Ø¨ Ø§Ù„Ù‚Ø±Ø§Ø± Ø¥Ù† ÙƒØ§Ù† Ø§Ù„ÙØ§Ø±Ù‚ Ø¨Ø³ÙŠØ· ÙˆØ§Ù„Ø¹Ø¨Ø§Ø±Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ© Ù…ÙˆØ¬ÙˆØ¯Ø©
        if has_neg and (probs[i_neu] - probs[i_neg] <= margin):
            return "negative"
        if has_pos and (probs[i_neu] - probs[i_pos] <= margin):
            return "positive"
    return label

# ------------------------
# Loaders
# ------------------------
@st.cache_resource(show_spinner=False)
def load_lang_assets(model_root: Path, lang: str):
    """Load tokenizer, label_map, and model for a language ('ar'|'en')."""
    lang_dir = model_root / lang
    if not lang_dir.exists():
        raise FileNotFoundError(f"Language folder not found: {lang_dir}")

    # tokenizer
    tok_path = lang_dir / "tokenizer.json"
    if tok_path.exists():
        with open(tok_path, "r", encoding="utf-8") as f:
            tok = tokenizer_from_json(f.read())
    else:
        raise FileNotFoundError(f"Missing tokenizer.json in {lang_dir}")

    # classes
    label_map_path = lang_dir / "label_map.json"
    if label_map_path.exists():
        try:
            with open(label_map_path, "r", encoding="utf-8") as f:
                classes = json.load(f)["classes"]
        except Exception:
            classes = CLASSES_FALLBACK
    else:
        classes = CLASSES_FALLBACK

    # model candidates
    candidates = [
        lang_dir / f"{lang}_best.keras",
        lang_dir / f"{lang}_final.keras",
        lang_dir / f"{lang}_best.h5",
        lang_dir / f"{lang}_final.h5",
        lang_dir / "saved_model",     # TF SavedModel directory
    ]
    model_path = next((p for p in candidates if p.exists()), None)
    if model_path is None:
        raise FileNotFoundError(
            f"No model file found in {lang_dir}. "
            f"Expected one of: {', '.join(p.name for p in candidates)}"
        )

    model = tf.keras.models.load_model(model_path)
    return tok, classes, model


def _predict_batch(texts: List[str], model_root: Path) -> pd.DataFrame:
    """Predict for a list of raw texts (AR + EN mixed)."""
    langs = ["ar" if ARABIC_RE.search(t or "") else "en" for t in texts]
    rows = []
    cache: Dict[str, Tuple[Any, Any, Any]] = {}

    for lang in ("ar", "en"):
        idxs = [i for i, l in enumerate(langs) if l == lang]
        if not idxs:
            continue

        if lang not in cache:
            tok, classes, model = load_lang_assets(model_root, lang)
            cache[lang] = (tok, classes, model)
        else:
            tok, classes, model = cache[lang]

        subset = [preprocess_text(texts[i], lang) for i in idxs]
        seq = tok.texts_to_sequences(subset)
        X = pad_sequences(seq, maxlen=MAX_LEN, padding="post", truncating="post")
        probs = model.predict(X, verbose=0)
        pred_idx = np.argmax(probs, axis=1)

        for j, i_global in enumerate(idxs):
            label = classes[int(pred_idx[j])] if int(pred_idx[j]) < len(classes) else str(int(pred_idx[j]))
            if lang == "ar":
                label = override_ar_prediction(texts[i_global], label, probs[j], classes)

            row = {
                "text": texts[i_global],
                "lang": lang,
                "label": label,
                "confidence": float(probs[j, pred_idx[j]]),
            }
            # add per-class probs if available
            for ci, cname in enumerate(classes):
                row[f"p_{cname}"] = float(probs[j, ci])
            rows.append(row)

    return pd.DataFrame(rows)


def save_uploaded_model_files(lang: str, files: Dict[str, Any], root: Path) -> str:
    """Save uploaded files for a given language folder."""
    lang_dir = root / lang
    lang_dir.mkdir(parents=True, exist_ok=True)

    if files.get("zip"):
        try:
            with zipfile.ZipFile(files["zip"]) as z:
                z.extractall(lang_dir)
            return f"âœ… Extracted into {lang_dir}"
        except Exception as e:
            return f"âš ï¸ ZIP extract failed: {e}"

    saved = []
    for key in ("model1", "model2"):
        f = files.get(key)
        if f and (f.name.endswith(".keras") or f.name.endswith(".h5")):
            out = lang_dir / f.name
            out.write_bytes(f.read())
            saved.append(out.name)

    tok = files.get("tokenizer")
    if tok:
        (lang_dir / "tokenizer.json").write_bytes(tok.read())
        saved.append("tokenizer.json")

    lmap = files.get("label_map")
    if lmap:
        (lang_dir / "label_map.json").write_bytes(lmap.read())
        saved.append("label_map.json")

    if not saved:
        return "âš ï¸ No files saved. Please upload model + tokenizer + label_map."
    return "âœ… Saved: " + ", ".join(saved)

# ------------------------
# File readers
# ------------------------
def read_pdf(file) -> List[str]:
    texts = []
    try:
        reader = PdfReader(file)
        for pg in reader.pages:
            t = pg.extract_text() or ""
            t = t.strip()
            if t:
                texts.append(t)
    except Exception as e:
        st.error(f"PDF read error: {e}")
    return texts

def read_docx(file) -> List[str]:
    texts = []
    try:
        doc = Document(file)
        for p in doc.paragraphs:
            t = (p.text or "").strip()
            if t:
                texts.append(t)
    except Exception as e:
        st.error(f"DOCX read error: {e}")
    return texts

def read_csv(file) -> pd.DataFrame:
    df = None
    for enc in ("utf-8", "utf-8-sig", "latin-1", "cp1256"):
        try:
            file.seek(0)
            df = pd.read_csv(file, encoding=enc)
            break
        except UnicodeDecodeError:
            continue
    if df is None:
        file.seek(0)
        df = pd.read_csv(file)
    return df

# ------------------------
# Sidebar
# ------------------------
with st.sidebar:
    st.header("âš™ï¸ Settings | Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª")
    model_root = Path(st.text_input("Model directory | Ù…Ø³Ø§Ø± Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„Ø§Øª", value=str(DEFAULT_MODEL_DIR)))
    st.caption("Structure:\n"
               "bilingual_sentiment_model/ar (model + tokenizer.json + label_map.json)\n"
               "bilingual_sentiment_model/en (model + tokenizer.json + label_map.json)")

# ------------------------
# Tabs
# ------------------------
st.title("ğŸ’¬ Sentiment Analysis | ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø´Ø§Ø¹Ø± (AR/EN)")
tabs = st.tabs([
    "ğŸ“ Single Text | Ù†Øµ ÙˆØ§Ø­Ø¯",
    "ğŸ“ File (CSV / PDF / DOCX) | Ù…Ù„Ù",
    "ğŸ§© Model Manager | Ø¥Ø¯Ø§Ø±Ø© Ø§Ù„Ù†Ù…Ø§Ø°Ø¬",
    "ğŸ©º Environment | Ø§Ù„Ø¨ÙŠØ¦Ø©"
])

# ---- Tab 1: Single text
with tabs[0]:
    st.subheader("Single Text Prediction | Ø§Ù„ØªÙ†Ø¨Ø¤ Ù„Ù†Øµ ÙˆØ§Ø­Ø¯")
    t = st.text_area("Enter text (Arabic or English) | Ø£Ø¯Ø®Ù„ Ù†Øµ (Ø¹Ø±Ø¨ÙŠ Ø£Ùˆ Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠ):",
                     height=140, placeholder="Ù…Ø«Ø§Ù„: Ø§Ù†Ø§ Ø³Ø¹ÙŠØ¯ Ø§Ù„ÙŠÙˆÙ… / I love this product")
    if st.button("Predict | ØªÙ†Ø¨Ø¤", type="primary"):
        if t.strip():
            try:
                df = _predict_batch([t], model_root)
                if df.empty:
                    st.warning("No output. Check models/files.")
                else:
                    row = df.iloc[0]
                    lang_badge = "ğŸ‡¸ğŸ‡¦ Arabic | Ø¹Ø±Ø¨ÙŠ" if row["lang"] == "ar" else "ğŸ‡¬ğŸ‡§ English | Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠ"
                    st.success(f"**Language | Ø§Ù„Ù„ØºØ©:** {lang_badge}\n\n"
                               f"**Prediction | Ø§Ù„Ù†ØªÙŠØ¬Ø©:** `{row['label']}`  |  "
                               f"**Confidence | Ø§Ù„Ø«Ù‚Ø©:** `{row['confidence']:.3f}`")
                    # show probs table if present
                    prob_cols = [c for c in df.columns if c.startswith("p_")]
                    if prob_cols:
                        st.markdown("**Probabilities | Ø§Ù„Ø§Ø­ØªÙ…Ø§Ù„Ø§Øª:**")
                        st.dataframe(df[prob_cols].T.rename(columns={0: "probability"}), use_container_width=True)
            except Exception as e:
                st.error("Error | Ø®Ø·Ø£:")
                st.exception(e)
        else:
            st.warning("Please enter some text | Ø§Ù„Ø±Ø¬Ø§Ø¡ Ø¥Ø¯Ø®Ø§Ù„ Ù†Øµ.")

# ---- Tab 2: File (CSV / PDF / DOCX)
with tabs[1]:
    st.subheader("Batch Prediction from file | Ø§Ù„ØªÙ†Ø¨Ø¤ Ù…Ù† Ù…Ù„Ù")
    up = st.file_uploader("Upload CSV (text column) or PDF or DOCX | Ø§Ø±ÙØ¹ CSV Ø£Ùˆ PDF Ø£Ùˆ DOCX",
                          type=["csv", "pdf", "docx"])
    run = st.button("Run | ØªØ´ØºÙŠÙ„", type="primary")
    if up and run:
        try:
            if up.name.lower().endswith(".csv"):
                df = read_csv(up)
                if "text" not in df.columns:
                    # Ø§Ø³Ù… Ø£ÙˆÙ„ Ø¹Ù…ÙˆØ¯ ÙŠØµØ¨Ø­ text Ù„Ùˆ Ù…Ø§ ÙÙŠÙ‡ Ø¹Ù…ÙˆØ¯ Ø¨Ø§Ø³Ù… text
                    df = df.rename(columns={df.columns[0]: "text"})
                texts = df["text"].astype(str).tolist()

            elif up.name.lower().endswith(".pdf"):
                texts = read_pdf(up)

            else:  # docx
                texts = read_docx(up)

            if not texts:
                st.warning("No text found | Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù†ØµÙˆØµ.")
            else:
                out_df = _predict_batch(texts, model_root)
                if up.name.lower().endswith(".csv"):
                    # Ø£Ù„Ø­Ù‚ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø¹Ù„Ù‰ CSV Ø§Ù„Ø£ØµÙ„ÙŠ
                    res = pd.concat([df.reset_index(drop=True), out_df.drop(columns=["text"]).reset_index(drop=True)], axis=1)
                else:
                    res = out_df

                st.success(f"Predicted {len(res)} rows | ØªÙ… Ø§Ù„ØªÙ†Ø¨Ø¤ Ù„Ø¹Ø¯Ø¯ {len(res)} ØµÙÙˆÙ")
                st.dataframe(res, use_container_width=True)
                st.download_button("Download results CSV | ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†ØªØ§Ø¦Ø¬",
                                   data=res.to_csv(index=False).encode("utf-8"),
                                   file_name="predictions.csv", mime="text/csv")
        except Exception as e:
            st.error("Error | Ø®Ø·Ø£:")
            st.exception(e)

# ---- Tab 3: Model Manager
with tabs[2]:
    st.subheader("Model Manager | Ø¥Ø¯Ø§Ø±Ø© Ø§Ù„Ù†Ù…Ø§Ø°Ø¬")
    st.caption("Upload model (.keras/.h5), tokenizer.json, and label_map.json for Arabic and English.\n"
               "Ø§Ø±ÙØ¹ Ù…Ù„ÙØ§Øª Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„ (.keras/.h5) Ùˆ tokenizer.json Ùˆ label_map.json Ù„ÙƒÙ„ Ù„ØºØ©.")
    col1, col2 = st.columns(2)
    for col, lang_label in zip((col1, col2), ("ar", "en")):
        with col:
            st.markdown(f"### {'Arabic ğŸ‡¸ğŸ‡¦ | Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©' if lang_label=='ar' else 'English ğŸ‡¬ğŸ‡§ | Ø§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ©'}")
            up_zip = st.file_uploader(f"[{lang_label}] ZIP (optional) | Ù…Ù„Ù ZIP (Ø§Ø®ØªÙŠØ§Ø±ÙŠ)", type=["zip"], key=f"{lang_label}_zip")
            up_model1 = st.file_uploader(f"[{lang_label}] Model file 1 | Ù…Ù„Ù Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„ 1 (.keras/.h5)", type=["keras","h5"], key=f"{lang_label}_m1")
            up_model2 = st.file_uploader(f"[{lang_label}] Model file 2 | Ù…Ù„Ù Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„ 2 (.keras/.h5)", type=["keras","h5"], key=f"{lang_label}_m2")
            up_tok   = st.file_uploader(f"[{lang_label}] tokenizer.json", type=["json"], key=f"{lang_label}_tok")
            up_map   = st.file_uploader(f"[{lang_label}] label_map.json", type=["json"], key=f"{lang_label}_map")
            if st.button(f"Save {lang_label.upper()} files | Ø­ÙØ¸ Ù…Ù„ÙØ§Øª {lang_label.upper()}", key=f"save_{lang_label}"):
                files = {"zip": up_zip, "model1": up_model1, "model2": up_model2, "tokenizer": up_tok, "label_map": up_map}
                msg = save_uploaded_model_files(lang_label, files, model_root)
                st.info(msg)

    st.markdown("**Expected folder structure | Ù‡ÙŠÙƒÙ„ Ø§Ù„Ù…Ø¬Ù„Ø¯ Ø§Ù„Ù…ØªÙˆÙ‚Ø¹:**")
    st.code("""bilingual_sentiment_model/
  â”œâ”€ ar/
  â”‚   â”œâ”€ ar_best.keras / ar_best.h5 / saved_model/
  â”‚   â”œâ”€ tokenizer.json
  â”‚   â””â”€ label_map.json
  â””â”€ en/
      â”œâ”€ en_best.keras / en_best.h5 / saved_model/
      â”œâ”€ tokenizer.json
      â””â”€ label_map.json
""")

# ---- Tab 4: Environment / Health
with tabs[3]:
    st.subheader("Environment check | ÙØ­Øµ Ø§Ù„Ø¨ÙŠØ¦Ø©")
    st.write("**Python:**", sys.version)
    try:
        st.write("**TensorFlow:**", tf.__version__)
        st.write("**Num GPUs:**", len(tf.config.list_physical_devices('GPU')))
    except Exception as e:
        st.error("TensorFlow import failed | ÙØ´Ù„ Ø§Ø³ØªÙŠØ±Ø§Ø¯ TensorFlow")
        st.exception(e)

    try:
        import numpy as np; import pandas as pd; import h5py
        st.write("**NumPy:**", np.__version__)
        st.write("**Pandas:**", pd.__version__)
        st.write("**h5py:**", h5py.__version__)
    except Exception as e:
        st.error("Base libs import failed")
        st.exception(e)

    st.write("**Model root exists?**", DEFAULT_MODEL_DIR.exists(), str(DEFAULT_MODEL_DIR.resolve()))
    for lang in ("ar","en"):
        d = DEFAULT_MODEL_DIR / lang
        st.write(f"**{lang} dir exists?**", d.exists(), str(d))
        if d.exists():
            try:
                st.code("\n".join([p.name for p in sorted(d.iterdir())]), language="bash")
            except Exception:
                pass
