# app.py â€” Bilingual (Arabic + English) Sentiment Analysis
# --------------------------------------------------------
# ÙŠØ¯Ø¹Ù…: Ù†Øµ ÙˆØ§Ø­Ø¯ØŒ CSVØŒ PDFØŒ DOCX + ÙØ­Øµ Ø§Ù„Ø¨ÙŠØ¦Ø©
# Ù…Ø¶Ø§Ù: Ù‚ÙˆØ§Ø¹Ø¯ Ø¹Ø±Ø¨ÙŠØ© Ù„ÙÙƒ Ø§Ù„Ø­ÙŠØ§Ø¯ (Ù†ÙÙŠ/ØªØ¹Ø¬Ø¨/Ù…ÙƒØ«Ù‘ÙØ§Øª/Ø¥ÙŠÙ…ÙˆØ¬ÙŠ)

import os, sys, re, json
from pathlib import Path
from typing import Dict, Any, Tuple, List

import numpy as np
import pandas as pd
import streamlit as st

# ---- PDF & DOCX readers (Ø§Ø®ØªÙŠØ§Ø±ÙŠ) ----
try:
    from pypdf import PdfReader
except Exception:
    PdfReader = None

try:
    from docx import Document
except Exception:
    Document = None

# ===== Lazy TensorFlow import =====
TF_IMPORT_ERROR = None
tf = None
tokenizer_from_json = None
pad_sequences = None

def ensure_tf():
    """Import TensorFlow only when needed."""
    global tf, tokenizer_from_json, pad_sequences, TF_IMPORT_ERROR
    if tf is not None:
        return True, None
    try:
        import tensorflow as _tf
        from tensorflow.keras.preprocessing.text import tokenizer_from_json as _tok_json
        from tensorflow.keras.preprocessing.sequence import pad_sequences as _pad
        tf = _tf
        tokenizer_from_json = _tok_json
        pad_sequences = _pad
        return True, None
    except Exception as e:
        TF_IMPORT_ERROR = e
        return False, str(e)

# ------------------------
# Config
# ------------------------
st.set_page_config(page_title="ğŸ’¬ Sentiment | ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø´Ø§Ø¹Ø±", page_icon="ğŸ’¬", layout="wide")
DEFAULT_MODEL_DIR = Path("bilingual_sentiment_model")
MAX_LEN = 96
CLASSES_FALLBACK = ["negative", "neutral", "positive"]

# ------------------------
# Lang utils
# ------------------------
ARABIC_RE = re.compile(r'[\u0600-\u06FF]')
AR_DIACRITICS = r"[\u0617-\u061A\u064B-\u0652\u0670]"

def detect_language_simple(text: str) -> str:
    return "ar" if ARABIC_RE.search(str(text)) else "en"

def ar_normalize(s: str) -> str:
    s = str(s)
    s = re.sub(AR_DIACRITICS, "", s)
    s = re.sub(r"[Ù€]+", "", s)
    s = s.replace("Ø£","Ø§").replace("Ø¥","Ø§").replace("Ø¢","Ø§")
    s = s.replace("Ù‰","ÙŠ").replace("Ø¤","Ùˆ").replace("Ø¦","ÙŠ").replace("Ø©","Ù‡")
    s = re.sub(r"\s+", " ", s).strip()
    return s

def preprocess_text(txt: str, lang: str) -> str:
    return ar_normalize(txt) if lang == "ar" else txt

# ------------------------
# Arabic rules/keywords to break neutrality
# ------------------------
AR_NEG = {
    "Ø­Ø²ÙŠÙ†","Ø²Ø¹Ù„Ø§Ù†","ØªØ¹ÙŠØ³","Ø³ÙŠØ¦","Ø³ÙŠØ¡","Ø³Ø¦","Ù…ÙƒØªØ¦Ø¨","Ù…Ø­Ø¨Ø·","ØªØ¹Ø¨Ø§Ù†","ÙƒØ§Ø±Ù‡",
    "Ù…Ø²Ø¹Ø¬","Ø±Ø¯ÙŠØ¡","Ø³ÙŠØ¦Ø©","ÙƒØ§Ø±Ø«ÙŠ","Ù…Ù‚Ø±Ù","ÙØ¸ÙŠØ¹","Ø²ÙØª","Ù…Ø®ÙŠÙ‘Ø¨","Ø£Ø³ÙˆØ£","Ù…Ù…Ù„",
    "ÙƒØ§Ø±Ø«Ø©","Ø±Ø¯Ø§Ø¡Ø©","ØºØ¨Ù†","Ù‚Ø±Ù","Ù†Ø¯Ù…Øª","ØªØ§ÙÙ‡","Ø³ÙŠØ¦ÙŠÙ†"
}
AR_POS = {
    "Ø³Ø¹ÙŠØ¯","Ù…Ø¨Ø³ÙˆØ·","ÙØ±Ø­Ø§Ù†","Ù…Ù…ØªØ§Ø²","Ø±Ø§Ø¦Ø¹","Ø¬Ù…ÙŠÙ„","Ø­Ù„Ùˆ","Ø§Ø­Ø¨","Ø£Ø­Ø¨",
    "Ø¹Ø¬Ø¨Ù†ÙŠ","Ù…Ø°Ù‡Ù„","Ù…Ø³Ø¹Ø¯","Ù‡Ø§ÙŠÙ„","ÙƒÙˆÙŠØ³","Ù…Ù…ØªØ§Ø²Ù‡","ØªØ­ÙÙ‡","Ø®ÙŠØ§Ù„ÙŠ",
    "ÙŠÙÙˆØ²","Ø­Ø¨ÙŠØª","Ø£ÙØ¶Ù„","Ù…Ø±Ø¶ÙŠ","Ù…Ø¨Ù‡Ø±","Ø±ÙˆØ¹Ø©","ÙŠØ¬Ù†Ù†","Ø±Ù‡ÙŠØ¨","Ù…Ø±Ù‡ Ø­Ù„Ùˆ","ÙØ®Ù…"
}
AR_NEGATIONS = {"Ù…Ùˆ","Ù…Ø´","Ù„ÙŠØ³","Ù…Ø§","Ù…Ùˆ Ù…Ø±Ù‡","Ù…Ù‡Ùˆ","Ù…Ù‡ÙˆØ¨","ÙˆÙ„Ø§"}
AR_INTENSIFIERS = {"Ø¬Ø¯Ø§Ù‹","Ø¬Ø¯Ù‹Ø§","Ù…Ø±Ù‡","Ù…Ø±Ø©","Ø¨Ø´ÙƒÙ„ ÙƒØ¨ÙŠØ±","Ù…Ø±Ø© ÙƒØ«ÙŠØ±","Ù‚ÙˆÙŠ"}
EMOJI_POS = {"ğŸ˜Š","ğŸ˜","ğŸ¤©","ğŸ˜","ğŸ‘","ğŸ’–","âœ¨","ğŸ‘","ğŸ¥°"}
EMOJI_NEG = {"ğŸ˜","ğŸ˜¡","ğŸ¤¬","ğŸ˜¢","ğŸ‘","ğŸ’”","ğŸ˜ ","ğŸ˜­"}

EXCLAMATION_BOOST = 0.06   # ØªØ¹Ø²ÙŠØ² Ø¨Ø³ÙŠØ· Ù„Ùˆ ÙÙŠ ! ÙƒØ«ÙŠØ±Ø©
INTENSIFIER_BOOST = 0.07   # ØªØ¹Ø²ÙŠØ² Ù„Ùˆ ÙÙŠ Ø¬Ø¯Ø§Ù‹/Ù…Ø±Ù‡
RULE_CONF = 0.55           # Ø«Ù‚Ø© Ø§ÙØªØ±Ø§Ø¶ÙŠØ© Ù„Ùˆ Ù‚Ù„Ø¨Ù†Ø§ Ø¨Ø§Ù„Ù‚Ø§Ø¹Ø¯Ø©
LOW_CONF = 0.60            # Ù„Ùˆ Ø£Ø¹Ù„Ù‰ Ø§Ø­ØªÙ…Ø§Ù„ Ø£Ù‚Ù„ Ù…Ù† Ù‡Ø°Ø§ Ù†Ø³Ù…Ø­ Ù„Ù„Ù‚Ø§Ø¹Ø¯Ø© ØªÙ‚Ù„Ø¨
NEU_MARGIN = 0.18          # Ø³Ù…Ø§Ø­ÙŠØ© Ù„ÙÙƒ Ø§Ù„Ø­ÙŠØ§Ø¯

def _rule_score_ar(text: str) -> str | None:
    """Ù‚ÙˆØ§Ø¹Ø¯ Ø³Ø±ÙŠØ¹Ø©: ØªÙØ±Ø¬ÙØ¹ 'positive' Ø£Ùˆ 'negative' Ø£Ùˆ None."""
    t = ar_normalize(text)
    has_pos = any(w in t for w in AR_POS) or any(e in text for e in EMOJI_POS)
    has_neg = any(w in t for w in AR_NEG) or any(e in text for e in EMOJI_NEG)

    # Ù†ÙÙŠ Ø¨Ø³ÙŠØ·: "Ù…Ùˆ Ø­Ù„Ùˆ" = Ø³Ù„Ø¨ÙŠ ØŒ "Ù…Ùˆ Ø³ÙŠØ¡" = Ø¥ÙŠØ¬Ø§Ø¨ÙŠ ØªÙ‚Ø±ÙŠØ¨Ø§Ù‹
    negation = any(n in t for n in AR_NEGATIONS)
    if negation:
        if has_pos and not has_neg:
            has_pos, has_neg = False, True
        elif has_neg and not has_pos:
            has_pos, has_neg = True, False

    if has_pos and not has_neg:
        return "positive"
    if has_neg and not has_pos:
        return "negative"
    return None

def override_ar_prediction(
    text: str,
    label: str,
    probs: np.ndarray,
    classes: List[str],
    margin: float = NEU_MARGIN
) -> tuple[str, float]:
    """ÙŠÙØ±Ø¬Ø¹ (label, confidence) Ø¨Ø¹Ø¯ Ø§Ù„Ù‚ÙˆØ§Ø¹Ø¯ ÙˆØ§Ù„ØªØ­Ø³ÙŠÙ†Ø§Øª Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©."""
    try:
        i_neg = classes.index("negative")
        i_neu = classes.index("neutral")
        i_pos = classes.index("positive")
    except ValueError:
        return label, float(np.max(probs))

    p_neg, p_neu, p_pos = float(probs[i_neg]), float(probs[i_neu]), float(probs[i_pos])

    # Ù„Ùˆ Ù…Ø­Ø§ÙŠØ¯ ÙˆØ¨Ø§Ù„Ù‚Ø±Ø¨ Ù…Ù† Ø£Ø­Ø¯ Ø§Ù„Ø·Ø±ÙÙŠÙ†ØŒ Ù†ÙÙƒ Ø§Ù„Ø­ÙŠØ§Ø¯
    if label == "neutral":
        if p_neu - p_neg <= margin:
            label = "negative"
        if p_neu - p_pos <= margin:
            label = "positive"

    # Ù‚ÙˆØ§Ø¹Ø¯ Ù„ØºÙˆÙŠØ©/Ø¥ÙŠÙ…ÙˆØ¬ÙŠ Ø¥Ø°Ø§ Ø§Ù„Ø«Ù‚Ø© Ø¶Ø¹ÙŠÙØ© Ø£Ùˆ Ù…Ø§ Ø²Ø§Ù„ Ù…Ø­Ø§ÙŠØ¯
    top_p = max(p_neg, p_neu, p_pos)
    rule = _rule_score_ar(text)
    if rule and (label == "neutral" or top_p < LOW_CONF):
        label = rule
        top_p = max(top_p, RULE_CONF)

    # ØªØ¹Ø²ÙŠØ² Ø­Ø³Ø¨ Ø¹Ù„Ø§Ù…Ø§Øª Ø§Ù„ØªØ¹Ø¬Ø¨ ÙˆØ§Ù„Ù…ÙƒØ«ÙÙ‘ÙØ§Øª
    boost = 0.0
    excl = text.count("!")
    if excl >= 2: boost += EXCLAMATION_BOOST
    if any(w in text for w in AR_INTENSIFIERS): boost += INTENSIFIER_BOOST
    if label == "positive" and boost > 0:
        top_p = min(0.99, top_p + boost)
    if label == "negative" and boost > 0 and excl >= 3:
        top_p = min(0.99, top_p + boost/2)

    return label, top_p

# ------------------------
# Loaders
# ------------------------
@st.cache_resource(show_spinner=False)
def load_lang_assets(model_root: Path, lang: str):
    ok, err = ensure_tf()
    if not ok:
        raise RuntimeError(f"TensorFlow import failed: {err}")

    lang_dir = Path(model_root) / lang
    if not lang_dir.exists():
        raise FileNotFoundError(f"Language folder not found: {lang_dir}")

    # tokenizer
    tok_path = lang_dir / "tokenizer.json"
    if not tok_path.exists():
        raise FileNotFoundError(f"Missing tokenizer.json in {lang_dir}")
    with open(tok_path, "r", encoding="utf-8") as f:
        tok = tokenizer_from_json(f.read())

    # label_map
    label_map_path = lang_dir / "label_map.json"
    if label_map_path.exists():
        try:
            with open(label_map_path, "r", encoding="utf-8") as f:
                classes = json.load(f)["classes"]
        except Exception:
            classes = CLASSES_FALLBACK
    else:
        classes = CLASSES_FALLBACK

    # model file
    candidates = [
        lang_dir / f"{lang}_best.keras",
        lang_dir / f"{lang}_final.keras",
        lang_dir / f"{lang}_best.h5",
        lang_dir / f"{lang}_final.h5",
        lang_dir / "saved_model",
    ]
    model_path = next((p for p in candidates if p.exists()), None)
    if model_path is None:
        raise FileNotFoundError(f"No model file found in {lang_dir}")

    model = tf.keras.models.load_model(model_path)
    return tok, classes, model

def _predict_batch(texts: List[str], model_root: Path) -> pd.DataFrame:
    ok, err = ensure_tf()
    if not ok:
        raise RuntimeError(f"TensorFlow import failed: {err}")

    langs = ["ar" if ARABIC_RE.search(t or "") else "en" for t in texts]
    rows = []
    cache: Dict[str, Tuple[Any, Any, Any]] = {}

    for lang in ("ar", "en"):
        idxs = [i for i, l in enumerate(langs) if l == lang]
        if not idxs:
            continue
        if lang not in cache:
            tok, classes, model = load_lang_assets(model_root, lang)
            cache[lang] = (tok, classes, model)
        else:
            tok, classes, model = cache[lang]

        subset = [preprocess_text(texts[i], lang) for i in idxs]
        seq = tok.texts_to_sequences(subset)
        X = pad_sequences(seq, maxlen=MAX_LEN, padding="post", truncating="post")
        probs = model.predict(X, verbose=0)
        pred_idx = np.argmax(probs, axis=1)

        for j, i_global in enumerate(idxs):
            base_label = classes[int(pred_idx[j])] if int(pred_idx[j]) < len(classes) else str(int(pred_idx[j]))
            conf = float(probs[j, pred_idx[j]])
            label = base_label

            if lang == "ar":
                label, conf = override_ar_prediction(texts[i_global], base_label, probs[j], classes)

            row = {
                "text": texts[i_global],
                "lang": lang,
                "label": label,
                "confidence": float(conf),
            }
            for ci, cname in enumerate(classes):
                row[f"p_{cname}"] = float(probs[j, ci])
            rows.append(row)
    return pd.DataFrame(rows)

# ------------------------
# Readers
# ------------------------
def read_pdf(file) -> List[str]:
    if PdfReader is None:
        raise RuntimeError("pypdf not installed")
    texts = []
    reader = PdfReader(file)
    for pg in reader.pages:
        t = (pg.extract_text() or "").strip()
        if t: texts.append(t)
    return texts

def read_docx(file) -> List[str]:
    if Document is None:
        raise RuntimeError("python-docx not installed")
    texts = []
    doc = Document(file)
    for p in doc.paragraphs:
        t = (p.text or "").strip()
        if t: texts.append(t)
    return texts

def read_csv(file) -> pd.DataFrame:
    df = None
    for enc in ("utf-8","utf-8-sig","latin-1","cp1256"):
        try:
            file.seek(0); df = pd.read_csv(file, encoding=enc); break
        except UnicodeDecodeError:
            continue
    return df if df is not None else pd.read_csv(file)

# ------------------------
# Sidebar
# ------------------------
with st.sidebar:
    st.header("âš™ï¸ Settings | Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª")
    model_root = Path(st.text_input("Model directory | Ù…Ø³Ø§Ø± Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„Ø§Øª", value=str(DEFAULT_MODEL_DIR)))
    st.caption("bilingual_sentiment_model/ar & /en each: model + tokenizer.json + label_map.json")

# ------------------------
# Tabs
# ------------------------
st.title("ğŸ’¬ Sentiment Analysis | ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø´Ø§Ø¹Ø± (AR/EN)")
tabs = st.tabs([
    "ğŸ“ Single Text | Ù†Øµ ÙˆØ§Ø­Ø¯",
    "ğŸ“ File (CSV / PDF / DOCX) | Ù…Ù„Ù",
    "ğŸ©º Environment | Ø§Ù„Ø¨ÙŠØ¦Ø©"
])

# ------------------------
# Tab 1 - Single text
# ------------------------
with tabs[0]:
    ok_tf, err_tf = ensure_tf()
    if not ok_tf:
        st.error("TensorFlow ØºÙŠØ± Ù…ØªØ§Ø­. Ø§ÙØªØ­ÙŠ ØªØ¨ÙˆÙŠØ¨ Environment Ù„Ù…Ø´Ø§Ù‡Ø¯Ø© Ø§Ù„Ø³Ø¨Ø¨.")
    else:
        t = st.text_area("Enter text (Arabic or English):", height=140,
                         placeholder="Ù…Ø«Ø§Ù„: Ø§Ù†Ø§ Ø³Ø¹ÙŠØ¯ Ø§Ù„ÙŠÙˆÙ… / I love this product")
        if st.button("Predict"):
            if t.strip():
                try:
                    df = _predict_batch([t], model_root)
                    row = df.iloc[0]
                    lang_badge = "ğŸ‡¸ğŸ‡¦ Ø¹Ø±Ø¨ÙŠ" if row["lang"] == "ar" else "ğŸ‡¬ğŸ‡§ English"
                    st.success(f"**Language:** {lang_badge}\n\n**Prediction:** `{row['label']}`  |  **Confidence:** `{row['confidence']:.3f}`")
                    prob_cols = [c for c in df.columns if c.startswith("p_")]
                    if prob_cols:
                        st.markdown("**Probabilities:**")
                        st.dataframe(df[prob_cols].T.rename(columns={0: "probability"}))
                except Exception as e:
                    st.error(str(e))
            else:
                st.warning("Ø§ÙƒØªØ¨ Ù†ØµÙ‹Ø§ Ø£ÙˆÙ„Ø§Ù‹.")

# ------------------------
# Tab 2 - File upload
# ------------------------
with tabs[1]:
    ok_tf, err_tf = ensure_tf()
    if not ok_tf:
        st.error("TensorFlow ØºÙŠØ± Ù…ØªØ§Ø­.")
    else:
        up = st.file_uploader("Upload CSV / PDF / DOCX", type=["csv","pdf","docx"])
        if st.button("Run") and up:
            try:
                if up.name.lower().endswith(".csv"):
                    df_in = read_csv(up)
                    if "text" not in df_in.columns:
                        df_in = df_in.rename(columns={df_in.columns[0]: "text"})
                    texts = df_in["text"].astype(str).tolist()
                elif up.name.lower().endswith(".pdf"):
                    texts = read_pdf(up)
                else:
                    texts = read_docx(up)

                out_df = _predict_batch(texts, model_root)
                st.dataframe(out_df, use_container_width=True)
                st.download_button("Download CSV", data=out_df.to_csv(index=False).encode("utf-8"),
                                   file_name="predictions.csv", mime="text/csv")
            except Exception as e:
                st.error(str(e))

# ------------------------
# Tab 3 - Environment
# ------------------------
with tabs[2]:
    st.write("**Python:**", sys.version)
    ok_tf, err_tf = ensure_tf()
    st.write("**TensorFlow imported?**", ok_tf)
    if ok_tf:
        st.write("TF version:", tf.__version__)
        st.write("Num GPUs:", len(tf.config.list_physical_devices('GPU')))
    else:
        st.error(err_tf)
    st.write("**Model root exists?**", DEFAULT_MODEL_DIR.exists(), str(DEFAULT_MODEL_DIR.resolve()))
    for lang in ("ar","en"):
        d = DEFAULT_MODEL_DIR / lang
        st.write(f"**{lang} folder exists?**", d.exists(), str(d))
        if d.exists():
            try:
                st.code("\n".join([p.name for p in sorted(d.iterdir())]), language="bash")
            except:
                pass
